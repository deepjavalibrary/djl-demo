{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with DJL Spark Support\n",
    "\n",
    "In this example, we will use Jupyter Notebook to run image Classification with DJL Spark extension on Scala. To execute this Scala kernel successfully, you need to install [Almond](https://almond.sh/), a Scala kernel for Jupyter Notebook. Almond provide extensive functionalities for Scala and Spark applications.\n",
    "\n",
    "[Almond installation instruction](https://almond.sh/docs/quick-start-install) (Note: only Scala 2.12 are tested)\n",
    "\n",
    "After that, you can start with DJL's Scala notebook.\n",
    "\n",
    "\n",
    "## Import dependencies\n",
    "\n",
    "Firstly, let's import the depdendencies we need. We choose to use DJL PyTorch as our backend engine. You can also switch to MXNet by uncommenting the two lines for MXNet and comment PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.2.2`\n",
    "import $ivy.`ai.djl:api:0.21.0`\n",
    "import $ivy.`ai.djl.spark:spark:0.21.0`\n",
    "import $ivy.`ai.djl.pytorch:pytorch-model-zoo:0.21.0`\n",
    "import $ivy.`ai.djl.pytorch:pytorch-native-cpu-precxx11:1.13.1`\n",
    "// import $ivy.`ai.djl.mxnet:mxnet-engine:0.21.0`\n",
    "// import $ivy.`ai.djl.mxnet:mxnet-native-mkl:1.9.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can import the packages we need to use. In the last two lines, we disabled the Spark logging in order to avoid polluting your cell outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.NotebookSparkSession\n",
    "import ai.djl.modality.Classifications\n",
    "import ai.djl.spark.SparkTransformer\n",
    "import ai.djl.spark.translator.SparkImageClassificationTranslator\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF) // avoid too much message popping out\n",
    "Logger.getLogger(\"ai\").setLevel(Level.OFF) // avoid too much message popping out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark application\n",
    "\n",
    "We can create a `NotebookSparkSession` through the Almond Spark plugin. It will internally apply all necessary jars to each of the worker node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create Spark session\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to load the images from the local folder using Spark library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.format(\"image\").option(\"dropInvalid\", true).load(\"../image-classification/images\")\n",
    "df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then We can run inference on these images. All we need to do is to create a `SparkTransformer` and run inference with DJL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "System.setProperty(\"PYTORCH_PRECXX11\", \"true\")\n",
    "val transformer = new SparkTransformer[Classifications]()\n",
    "  .setInputCol(\"image.*\")\n",
    "  .setOutputCol(\"value\")\n",
    "  .setModelUrl(\"https://alpha-djl-demos.s3.amazonaws.com/model/djl-blockrunner/pytorch_resnet18.zip?model_name=traced_resnet18\")\n",
    "  .setOutputClass(classOf[Classifications])\n",
    "  .setTranslator(new SparkImageClassificationTranslator())\n",
    "val outputDf = transformer.transform(df)\n",
    "println(outputDf.collect().mkString(\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
