{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "993dcf59-dd85-4a31-a8cc-02c5d4be5dfe",
   "metadata": {},
   "source": [
    "# Deploy OpenAI gpt-oss model on SageMaker AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef819c-5aff-4e14-a07d-27c227204560",
   "metadata": {},
   "source": [
    "In this notebook we deploy OpenAI gpt-oss model on SageMaker AI using S3 and LMI v16.\n",
    "\n",
    "Please see the OpenAI introduction [blog](https://simonwillison.net/2025/Aug/5/gpt-oss/) for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23a1755-ddcb-416c-aa74-33920280fed2",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "Fetch and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01a4b2-b0d3-420c-8335-4efd984bb5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sagemaker boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183bf15-3f3f-4d70-91da-fe269ff421b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4039c65-7c85-496b-9f4e-9b7d29bacdb5",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef2290-12be-486d-9dd1-a3f53c71e013",
   "metadata": {},
   "source": [
    "### Deploy using S3\n",
    "\n",
    "In this example, we will deploy a model from S3 to the LMI container. In order to make this easier, we will use the already existing weights from the JumpStart cache.\n",
    "You can search for them directly using the following command:\n",
    "\n",
    "`aws s3 ls s3://jumpstart-cache-prod-YOUR_REGION/ --recursive | grep YOUR_MODEL_NAME`",
    "\n",
    "Disclaimer: Be sure to run the above command to ensure that the model you want is avaiable on Jumpstart in your expected region. Otherwise the model weights will need to be copied into your S3 bucket manually",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c4454-db44-43cf-8e6b-91f9d687dcde",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# copy the jumpstart weights for gpt-oss into the sagemaker bucket\n",
    "!aws s3 sync s3://jumpstart-cache-prod-{region}/openai-reasoning/openai-reasoning-gpt-oss-20b/artifacts/inference-prepack/v1.0.0 s3://{bucket}/gpt-oss-20b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c444fe89-1c22-420c-a239-5ecd65ee375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_image = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.34.0-lmi16.0.0-cu128-v1.1\"\n",
    "\n",
    "instance_type = \"ml.g6.12xlarge\"\n",
    "num_gpu = 4\n",
    "model_s3_path = f\"s3://{bucket}/gpt-oss-20b/\"\n",
    "\n",
    "lmi_env = {\n",
    "    \"OPTION_MODEL_ID\": \"/opt/ml/model\",\n",
    "    \"OPTION_ASYNC_MODE\": \"true\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\":\"8\",\n",
    "    \"OPTION_MAX_MODEL_LEN\":\"1500\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"disable\",\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.lmi_vllm.vllm_async_service\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "}\n",
    "\n",
    "model_name = sagemaker.utils.name_from_base(\"model-lmi\")\n",
    "endpoint_name = model_name\n",
    "inference_component_name = f\"ic-{model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80842d14-df48-47b2-ade6-e121ebc6f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n",
    "\n",
    "lmi_model = sagemaker.Model(\n",
    "    image_uri=inference_image,\n",
    "    env=lmi_env,\n",
    "    role=role,\n",
    "    name=model_name,\n",
    "    model_data={\n",
    "        'S3DataSource': {\n",
    "            'S3Uri': model_s3_path,\n",
    "            'S3DataType': 'S3Prefix',\n",
    "            'CompressionType': 'None'\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "lmi_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    container_startup_health_check_timeout=600,\n",
    "    endpoint_name=endpoint_name,\n",
    "    endpoint_type=sagemaker.enums.EndpointType.INFERENCE_COMPONENT_BASED,\n",
    "    inference_component_name=inference_component_name,\n",
    "    resources=ResourceRequirements(requests={\"num_accelerators\": num_gpu, \"memory\": 1024*5, \"copies\": 1,}),\n",
    ")\n",
    "\n",
    "llm = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    "    component_name=inference_component_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "63e5ad21-d463-457e-af08-50e623c64223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "Hello! How can I help you today?\n",
      "-----\n",
      "\n",
      "{'prompt_tokens': 70, 'total_tokens': 102, 'completion_tokens': 32, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "request = {\n",
    "        \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": \"hi\"}\n",
    "              ],\n",
    "              \"max_tokens\": 50,\n",
    "              \"temperature\": 0.75,\n",
    "              \"stop\": None\n",
    "            }\n",
    "res = llm.predict(request)\n",
    "print(\"-----\\n\" + res[\"choices\"][0][\"message\"][\"content\"] + \"\\n-----\\n\")\n",
    "print(res[\"usage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97335e2-65eb-4571-83dd-1cb7fae9bf9a",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90af74a3-54f2-4576-8fac-9095ebb086cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-10T19:55:56.924769Z",
     "iopub.status.busy": "2025-08-10T19:55:56.924431Z",
     "iopub.status.idle": "2025-08-10T19:55:57.689309Z",
     "shell.execute_reply": "2025-08-10T19:55:57.688779Z",
     "shell.execute_reply.started": "2025-08-10T19:55:56.924747Z"
    }
   },
   "outputs": [],
   "source": [
    "sess.delete_inference_component(inference_component_name)\n",
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "sess.delete_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
